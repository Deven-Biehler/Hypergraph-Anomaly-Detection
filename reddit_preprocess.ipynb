{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "percent_anomalous_users = 0.03\n",
    "percent_anomalous_forums = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def subset_data(filename, output_file, length):\n",
    "    output = []\n",
    "    for i, line in enumerate(open(filename)):\n",
    "        if i >= length:\n",
    "            break\n",
    "        line = json.loads(line)\n",
    "        line[\"label\"] = 0\n",
    "        if line[\"author\"] == \"[deleted]\" or len(line[\"body\"]) > 500:\n",
    "            length += 1\n",
    "            continue\n",
    "        output.append(line)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(json.dumps(line) + \"\\n\")\n",
    "    print(\"Subsetting Done\")\n",
    "    print(\"Number of lines: \", len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_user_data(input_file_path, output_file_path):\n",
    "    user_data = {}\n",
    "    # Example of a line in the input file: author: [(subreddit, body, label), (subreddit, body, label), ...]\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line_dict = json.loads(line)\n",
    "            author = line_dict[\"author\"]\n",
    "            if author not in user_data:\n",
    "                user_data[author] = []\n",
    "            user_data[author].append((line_dict[\"subreddit\"], line_dict[\"body\"], line_dict[\"label\"]))\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        json.dump(user_data, file, indent = 4)\n",
    "    print(\"Preprocessing complete\")\n",
    "    print(\"Number of users: \" + str(len(user_data.keys())))\n",
    "    return user_data\n",
    "\n",
    "def get_subreddit_data(input_file_path, output_file_path):\n",
    "    subreddit_data = {}\n",
    "    # Example of a line in the input file: subreddit: [(author, body, label), (author, body, label), ...]\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line_dict = json.loads(line)\n",
    "            subreddit = line_dict[\"subreddit\"]\n",
    "            if subreddit not in subreddit_data:\n",
    "                subreddit_data[subreddit] = []\n",
    "            subreddit_data[subreddit].append((line_dict[\"author\"], line_dict[\"body\"], line_dict[\"label\"]))\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        json.dump(subreddit_data, file, indent = 4)\n",
    "    print(\"Preprocessing complete\")\n",
    "    print(\"Number of subreddits: \" + str(len(subreddit_data.keys())))\n",
    "    return subreddit_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting Done\n",
      "Number of lines:  30000\n"
     ]
    }
   ],
   "source": [
    "subset_data(\"data\\Reddit\\RC_2015-01\", \"data\\Reddit\\Reddit_subset.json\", 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete\n",
      "Number of users: 20046\n"
     ]
    }
   ],
   "source": [
    "clean_user_data = get_user_data(\"data\\Reddit\\Reddit_subset.json\", \"data\\Reddit\\Reddit_user_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete\n",
      "Number of subreddits: 3055\n"
     ]
    }
   ],
   "source": [
    "subreddit_data = get_subreddit_data(\"data\\Reddit\\Reddit_subset.json\", \"data\\Reddit\\Reddit_subreddit_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\biehl\\AppData\\Local\\Temp\\ipykernel_18956\\379416001.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  scammers = random.sample(clean_user_data.keys(), int(len(clean_user_data.keys()) * percent_anomalous_users))\n",
      "C:\\Users\\biehl\\AppData\\Local\\Temp\\ipykernel_18956\\379416001.py:4: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  scam_subreddits = random.sample(subreddit_data.keys(), 1 + int(len(subreddit_data.keys()) * percent_anomalous_forums))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Randomly select anomalous users and subreddits\n",
    "scammers = random.sample(clean_user_data.keys(), int(len(clean_user_data.keys()) * percent_anomalous_users))\n",
    "scam_subreddits = random.sample(subreddit_data.keys(), 1 + int(len(subreddit_data.keys()) * percent_anomalous_forums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalous data:  9041\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "email_spam = pd.read_csv(\"data\\Reddit\\processed_data.csv\")\n",
    "# Assuming the data is stored in a DataFrame called \"df\"\n",
    "email_spam = email_spam[email_spam['message'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "anomalous_data = email_spam[email_spam[\"label\"] == 1]\n",
    "anomalous_data = anomalous_data[\"message\"].values.tolist()\n",
    "anomalous_data_filtered = []\n",
    "for i, message in enumerate(anomalous_data):\n",
    "    if len(message) < 500 and len(message) > 5:\n",
    "        anomalous_data_filtered.append(message)\n",
    "print(\"Number of anomalous data: \", len(anomalous_data_filtered))\n",
    "\n",
    "import random\n",
    "def inject_anomalies(clean_user_data, anomaly_data, scammers, scam_subreddits):\n",
    "    injected = 0\n",
    "    inject_user_data = {}\n",
    "    for user in clean_user_data:\n",
    "        for i, _ in enumerate(clean_user_data[user]):\n",
    "            if user not in inject_user_data:\n",
    "                inject_user_data[user] = []\n",
    "            if user in scammers:\n",
    "                inject_user_data[user].append((random.choice(scam_subreddits), # Subreddit\n",
    "                        anomaly_data.pop(), # Body\n",
    "                        1 # Label\n",
    "                        ))\n",
    "            else:\n",
    "                inject_user_data[user].append(clean_user_data[user][i])\n",
    "            injected += 1\n",
    "    print(\"Anomalies injected\")\n",
    "    print(\"Number of anomalies injected: \", injected)\n",
    "    return inject_user_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies injected\n",
      "Number of anomalies injected:  30000\n",
      "GML file created\n",
      "GML file created\n"
     ]
    }
   ],
   "source": [
    "user_data = inject_anomalies(clean_user_data, anomalous_data_filtered, scammers, scam_subreddits)\n",
    "import networkx as nx\n",
    "def create_gml(data, output_file):\n",
    "    G = nx.Graph()\n",
    "    for user, post in data.items():\n",
    "        for edge in post:\n",
    "            G.add_edge(user, edge[0], attr=edge[2])\n",
    "    nx.write_edgelist(G, \"data/Reddit/edgelist_attr.txt\")\n",
    "    G = nx.bipartite.read_edgelist(\"data/Reddit/edgelist_attr.txt\", create_using=nx.DiGraph)\n",
    "    nx.write_gml(G, output_file)\n",
    "    print(\"GML file created\")\n",
    "\n",
    "create_gml(clean_user_data, \"data\\Reddit\\Reddit_clean.gml\")\n",
    "create_gml(user_data, \"data\\Reddit\\Reddit.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Filter messages labeled 0 and 1\n",
    "# messages_0 = data[data['label'] == 0]['body']\n",
    "# messages_1 = data[data['label'] == 1]['body']\n",
    "\n",
    "# # Concatenate all messages into a single string\n",
    "# text_0 = ' '.join(messages_0)\n",
    "# text_1 = ' '.join(messages_1)\n",
    "\n",
    "# # Generate word clouds for each label\n",
    "# wordcloud_0 = WordCloud(background_color = \"white\").generate(text_0)\n",
    "# wordcloud_1 = WordCloud(background_color = \"white\").generate(text_1)\n",
    "\n",
    "# # Plot the word clouds\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# axes[0].imshow(wordcloud_0, interpolation='bilinear')\n",
    "# axes[0].set_title('Normal Text')\n",
    "# axes[0].axis('off')\n",
    "\n",
    "# axes[1].imshow(wordcloud_1, interpolation='bilinear')\n",
    "# axes[1].set_title('Anomalous Text')\n",
    "# axes[1].axis('off')\n",
    "\n",
    "# # Assuming you have a plot named 'fig'\n",
    "# plt.savefig('wordcloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_edglist(user_data, output_file):\n",
    "#     subreddit_list = []\n",
    "#     edgelist = []\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         for user_id, (_, post) in enumerate(user_data.items()):\n",
    "#             for edge in post:\n",
    "#                 if edge[0] not in subreddit_list:\n",
    "#                     subreddit_list.append(edge[0])\n",
    "#                 reddit_id = subreddit_list.index(edge[0])\n",
    "#                 edgelist.append((user_id, reddit_id))\n",
    "#         json.dump(edgelist, f, indent = 4)\n",
    "#     print(\"Edgelist file created\")\n",
    "# write_edglist(user_data, \"data\\Reddit\\edgelist.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph file created\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_hypergraph_edgelist(user_data):\n",
    "    hypergraph = {}\n",
    "    for user_id, posts in enumerate(user_data.values()):\n",
    "        for subreddit, _, _ in posts:\n",
    "            if subreddit not in hypergraph.keys():\n",
    "                hypergraph[subreddit] = []\n",
    "            hypergraph[subreddit].append(user_id)\n",
    "    with open(\"data/Reddit/hypergraph.json\", \"w\") as f:\n",
    "        json.dump(list(hypergraph.values()), f, indent = 4)\n",
    "    print(\"Hypergraph file created\")\n",
    "create_hypergraph_edgelist(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(user_data):\n",
    "    labels = []\n",
    "    for user, posts in user_data.items():\n",
    "        _, _, label = posts[0]\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "import json\n",
    "json.dump(get_labels(user_data), open(\"data/Reddit/labels.json\", \"w\"), indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def get_all_text(user_data, output_file_path):\n",
    "    all_text = []\n",
    "    all_text_labeled = pd.DataFrame(columns = [\"v1\", \"v2\"])\n",
    "    for user, posts in user_data.items():\n",
    "        label = posts[0][2]\n",
    "        text = \"\"\n",
    "        for (_, body, label) in posts:\n",
    "            text += body + \" \"\n",
    "        text = clean_text(text)\n",
    "        all_text.append(text)\n",
    "        all_text_labeled = pd.concat([all_text_labeled, pd.DataFrame({\"v1\": [label], \"v2\": [text]})])\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        json.dump(all_text, f, indent = 4)\n",
    "    with open(\"data/Reddit/all_text_labeled.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        all_text_labeled.to_csv(f, index = False)\n",
    "\n",
    "get_all_text(user_data, \"data/Reddit/all_text.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bag_of_words(input_file_path, output_file_path):\n",
    "    data = open(input_file_path, \"r\", encoding=\"utf-8\")\n",
    "    data = json.load(data)\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "    vectorizer.fit(data)\n",
    "    \n",
    "    bow = vectorizer.transform(data)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(bow, file)\n",
    "    return bow\n",
    "\n",
    "bow = bag_of_words(\"data/Reddit/all_text.json\", \"data/Reddit/features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_mask(input_file_path, output_file_path):\n",
    "    data = open(input_file_path, \"r\")\n",
    "    data = json.load(data)\n",
    "\n",
    "    train_mask = []\n",
    "    for i, _ in enumerate(data):\n",
    "        if i % 6 == 0 or i % 6 == 1 or i % 6 == 2 or i % 6 == 3:\n",
    "            train_mask.append(1)\n",
    "        else:\n",
    "            train_mask.append(0)\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "    json.dump(train_mask, output_file, indent = 4)\n",
    "\n",
    "def get_val_mask(input_file_path, output_file_path):\n",
    "    data = open(input_file_path, \"r\")\n",
    "    data = json.load(data)\n",
    "\n",
    "    val_mask = []\n",
    "    for i, user in enumerate(data):\n",
    "        if i % 6 == 4:\n",
    "            val_mask.append(1)\n",
    "        else:\n",
    "            val_mask.append(0)\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "    json.dump(val_mask, output_file, indent = 4)\n",
    "\n",
    "def get_test_mask(input_file_path, output_file_path):\n",
    "    data = open(input_file_path, \"r\")\n",
    "    data = json.load(data)\n",
    "\n",
    "    test_mask = []\n",
    "    for i, user in enumerate(data):\n",
    "        if i % 6 == 5:\n",
    "            test_mask.append(1)\n",
    "        else:\n",
    "            test_mask.append(0)\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "    json.dump(test_mask, output_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(input_file_path, output_file_path):\n",
    "    data = open(input_file_path, \"r\")\n",
    "    data = json.load(data)\n",
    "    labels = []\n",
    "    for label in data.values():\n",
    "        labels.append(label)\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "    json.dump(labels, output_file, indent = 4)\n",
    "# import dhg\n",
    "# import json\n",
    "# def get_labels(input_file_path, output_file_path):\n",
    "#     edgelist = json.load(open(input_file_path, \"r\"))\n",
    "#     G = dhg.BiGraph(len(user_data), len(subreddit_data), edgelist)\n",
    "#     H = dhg.Hypergraph.from_bigraph(G, U_as_vertex=True)\n",
    "#     H.draw()\n",
    "\n",
    "# get_labels(\"data/Reddit/edgelist.json\", \"data/Reddit/labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_mask(\"data/Reddit/labels.json\", \"data/Reddit/test_mask.json\")\n",
    "get_val_mask(\"data/Reddit/labels.json\", \"data/Reddit/val_mask.json\")\n",
    "get_train_mask(\"data/Reddit/labels.json\", \"data/Reddit/train_mask.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5 hashes:\n",
      "data/Reddit/features.pkl : 1112fc3984449922d719056492acca20\n",
      "MD5 hashes:\n",
      "data/Reddit/edgelist.pkl : e025fa72919d104738b9e68f0e6cb26f\n",
      "MD5 hashes:\n",
      "data/Reddit/labels.pkl : b4dd2b0082659821330ed9d681f52549\n",
      "MD5 hashes:\n",
      "data/Reddit/train_mask.pkl : fedd0650f731ac39b2e09d7a13b951f9\n",
      "MD5 hashes:\n",
      "data/Reddit/val_mask.pkl : e24dd1c41d9a425144f17b6a64b3acc8\n",
      "MD5 hashes:\n",
      "data/Reddit/test_mask.pkl : b1fe02b4bc6a4991bff77aebba28961f\n",
      "num_classes: 2\n",
      "num_vertices: 20046\n",
      "num_edges: 3017\n",
      "dim_features: 38820\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "def json_to_pkl(input_file_path, output_file_path):\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    with open(output_file_path, \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def hash(file_name):\n",
    "    md5_dict = {}\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = f.read()\n",
    "        md5_hash = hashlib.md5(data).hexdigest()\n",
    "        md5_dict[file_name] = md5_hash\n",
    "    print(\"MD5 hashes:\")\n",
    "    for file_name, md5_hash in md5_dict.items():\n",
    "        print(file_name, \":\", md5_hash)\n",
    "\n",
    "json_to_pkl(\"data/Reddit/labels.json\", \"data/Reddit/labels.pkl\")\n",
    "json_to_pkl(\"data/Reddit/hypergraph.json\", \"data/Reddit/edgelist.pkl\")\n",
    "json_to_pkl(\"data/Reddit/train_mask.json\", \"data/Reddit/train_mask.pkl\")\n",
    "json_to_pkl(\"data/Reddit/val_mask.json\", \"data/Reddit/val_mask.pkl\")\n",
    "json_to_pkl(\"data/Reddit/test_mask.json\", \"data/Reddit/test_mask.pkl\")\n",
    "\n",
    "hash(\"data/Reddit/features.pkl\")\n",
    "hash(\"data/Reddit/edgelist.pkl\")\n",
    "hash(\"data/Reddit/labels.pkl\")\n",
    "hash(\"data/Reddit/train_mask.pkl\")\n",
    "hash(\"data/Reddit/val_mask.pkl\")\n",
    "hash(\"data/Reddit/test_mask.pkl\")\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "labels = json.load(open(\"data/Reddit/labels.json\"))\n",
    "num_vertices = len(labels)\n",
    "edge_list = json.load(open(\"data/Reddit/hypergraph.json\"))\n",
    "num_edges = len(edge_list)\n",
    "dim_features = bow.shape[1]\n",
    "\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"num_vertices:\", num_vertices)\n",
    "print(\"num_edges:\", num_edges)\n",
    "print(\"dim_features:\", dim_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
